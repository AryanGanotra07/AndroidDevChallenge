#AndroidDevChallenge
#AndroidDevChallenge

Cover letter included in the repository named “AndroidDevChallenge.pdf”

The idea is to help visually impaired people to listen to what is in front of them or what they can’t see. Instead of the stick they carry to guess where any object is,
holding an android app with camera access to detect objects in front of them and describe the object through a voice. Using machine learning’s various aspects like real time object detection technique, pose estimation, text recognition, landmark recognition, I want to create an android app that would detect the surroundings of the visually impaired person and describe them through voice output - text to speech engine. 

This app will be also capable to solve their day to day problems like :
Looking for lost objects. Example - wallet, cap, goggles.
Will use optical character recognition techniques or text recognition to read newspaper or some form’s details.

How machine learning will be used?
This app will be completely based on the real time object detection and tracking along with text and place/landmark recognition to identify the surrounding objects. Pose estimation techniques to identify the behavior of the people in the surrounding to avoid collisions or to ask for help and also for security purposes. Machine learning models will be trained on a regular basis using the the places visited and the tasks done by the user.

Thinking about how a visually impaired person will open the app to use it?
Google assistant has an answer to every problem. :)
The user will be able to open the app and use it by asking google assistant to open the app and the rest the app will manage itself taking care of everything without much user interactions.

Some example dialog flow

**While Looking for objects**

“There is an apple in front of you”.
“A hat is on slight left”.
“A bag is on slight right”.

**While walking**

“There is a building in front”.
“The board on slight left reads ‘Construction work ahead’ ”.
“Two people are present in front of you”.
“Oh there we got a dog”.
“There is a bench ahead”.


Tell us how you plan on bringing it to life. 

My project is at the concept phase for now. I am currently planning on what all and how to implement, also which technologies or machine learning tools will be best for this app.

Features to be implemented
Real time object detection and position tracker.
Text recognition.
Landmark recognition and pose estimator.
Voice outputs - Text to speech engine.
Linked with Google assistant using deep links.

Tools to be used 
Detect and track objects.
Tensorflow lite’s pose estimation.
Text recognition.
Place/Landmarks recognition.

A big thanks to Google’s ML Kit and TensorFlow lite to provide all the required tools with proper optimisation.

